********************************************************** k8s Architecture ******************************************************************

* k8s architecture mainly has 2 components: Master Node and Worker Node 

* Master Node : master node conatains etcd , kube-apiserver, kube-scheduler , kube controller manager and cloud controller manager 

-> kube-apiserver: It used to interact with k8s cluster and exposes the k8s apis . kubectl, user , master node components like kube controller manager and worker node components like kubelet everything will talk with apiserver.

-> etcd: It is like database for k8s cluster . It stores all the master and worker nodes data inside it. Like the amount of space in each node , amount of space used by each node and amount of space left in the node .

-> kube-scheduler: It will schedule all the pods and also distributes all the containes accross the nodes. It will also assign pods to nodes. 

-> kube controller manager: It is responsible for noticing and responding when node or container goes down . It will make decisions to make the containers or nodes up. Manages deployments and rollout of pods. When ever a pod goes down the kube-controller-manager notices the drop (via the API server) and spawns a new pod to meet the replica requirement.

-> cloud controller manager: It is not present for on-premsis k8s cluster . It will present in cloud-providers only . If a node is not responding it will check in the cloud provider that the node is deleted or not.

* Worker Node : kubelet and kubeproxy 

-> kubelet: It runs in every node in cluster . It will make sure that all the containers are running in a pod on node.

-> kube-proxy: It maintains network rules on nodes. It will allow network communication to pods. It will route the traffic to the right pods.

* Both the master and worker node has container runtime , where all the k8s components runs.

* When ever a cluster is created , bydefault 4 namespaces will be created default,kube-system,kube-public,kube-node-lease


************************************************************************** pods *****************************************************************

* Pod is single instance of an application.

* Pod is a smallest object that we can create in k8s.

* k8s will directly dont deploy containers. Containers are encapsulated into k8s pods.

* To scale up we will create a new pod and to scale down we will delete a pod.

* We cannot have multiple containers of same kind in a single pod.

* We can have multiple containers of differnet type in a single pod.

* Communication between the containers is easy , as they share the same network space.

* kubectl get pods -o wide -> will display extra info for that pod like on which node it is running.

* If any error with the pod use kubectl describe pod pod-name , under this event section you can see logs . By checking this logs we can debug the error.

* kubectl logs <pod-name> will give the logs of that particular pod

* To debug by entering into container of a pod use -> kubectl exec -it pod-name -- /bin/bash

* kubectl expose pod first-pod --type=LoadBalancer --port=80 --target-port=5000 --name=first-service -> to expose any pod as service 


************************************************************************** Services *******************************************************

* To expose the application which is running on pod in k8s , we use services.

* Cluster ip service is used for internal communication between applications in k8s cluster , with a stable internal ip.

* Cluster ip service create a ip inside the cluster , to enable communicate with other front-end services or back-end service.

* In k8s communication between pods within the same node will be done through service i.e cluster ip service.

* For nodeport service there will be three ports 
  1. Target port -> this is the port of the pod(container)
  2. port ->   this is the port of service
  3. Nodeport -> this is the port of node (range is 30000-32767)
  
* To access the node port in browser we can use http:ip of the node:nodeport. Ex: suppose we have a pod running in node of ip 172.16.22.22 and the nodeport of that node is 30008 then use http:172.16.22.22:30008

* The range of nodeports is 30000-32767 
 
* suppose the pods are distributed accross the different nodes then we can access with any node ip and its nodeport .
 
* kubectl describe service service-name -> will give the specific service details , can be useful for debugging.
 
* A Headless Service in Kubernetes is a special type of service that does not assign a cluster IP (i.e., no load balancing). Instead of exposing a single IP address, it directly returns the IP addresses of the individual pods backing the service.
 
* Headless service is mainly used for stateful applications like databases (e.g., MySQL, Cassandra) where clients need to connect to specific pod instances.
 
* To create a Headless service use clusterIp: none

* Ingress is a advanced loadbalancer service which provides context path based routing .

* Ingress acts as a entry point for external traffic into the k8s cluster and manages access to the services within the cluster.
 
* Ingress service provides a way to expose multiple services using a single external IP and route traffic based on hostnames or paths.

* Why we use ingress means Creating a LoadBalancer service for each app (costly) and also Expose services via NodePort (less secure and harder to manage).

* Single application	                     LoadBalancer
  Microservices architecture	             Ingress (with an Ingress Controller)
  Internal communication between service     ClusterIP (default)




 
 
************************************************************************* Deployment *******************************************************

* A Deployment in Kubernetes is a declarative API object that defines the desired state for running one or more identical Pods (through an underlying ReplicaSet) and manages their lifecycle—handling pod creation, updates (rolling updates), scaling, and rollbacks—so you don’t have to manually orchestrate individual Pods.

* kubectl create deployment first-deployment --image=stacksimplify/kubenginx:1.0.0

* To scale the rs use -> kubectl scale --replicas=2 deployment/first-deployment

* To expose deployment use -> kubectl expose deployment first-deployment --type=LoadBalancer --port=80 --target-port=80 --name=first-deployment-service

* To set the new image name use -> kubectl set image deployment/first-deployment container-name=stacksimplify/kubenginx:2.0.0 --record=true 

* To check the rollout history use -> kubectl rollout history deployment/first-deployments

* If you want to edit the deployment use -> kubectl edit deployment deployment-name

* In deployment rollback there are 2 options , 1.rollback to previous version 2.rollback to specific version 

* To rollout your deployment to prevoius version use -> kubectl rollout undo deployment/deployment-name

* To rollout to specific vsersion use -> kubectl rollout undo deployment/deployment-name --to-revision=revision-number

* To restart the pods in the deployment use -> kubectl rollout restart deployment/first-deployment

* To pause the deployment use -> kubectl rollout pause deployment/first-deployment

* To resume the deployment use -> kubectl rollout resume deployment/first-deployment




************************************************************************** Deployment rollback ***********************************************




* kubectl rollout undo deployment <deployment-name> This will rollout the deployment to previous version

* kubectl rollout history deployment <deployment-name> To see details of a specific revision:

* kubectl rollout deployment <deployment-name> --revision=<revision-number>



  
  
************************************************************************** Replica sets *******************************************************

* Replica set purpose is to a stable set of replica pods running at any given time.

* If any application crashes(Pods died) replica set will create the pod immediately to ensure that the configured number of pods are running.

* When load becomes more on the existing pods , k8s enables us to easily scale the application , adding additional pods as needed. This is going to seamless and fast.
  
* To check for any errors or logs use -> kubectl describe rs rs-name.

* You want to run a fixed set of pods and manage their lifecycle manually without using rolling updates, version tracking, or rollbacks — which are built-in features of a Deployment.

📌 Example Scenario:
You’re running a batch processing job or a stateless debug/test app where:

You don't want version history.

You don’t need rolling updates.

You want tight control over the pod definitions.

You want to start N identical pods and delete the ReplicaSet when done.



************************************************************************ Persistant volumes  ***********************************************

* A Persistent Volume is a piece of storage in the cluster. It’s created and managed by the cluster administrator.

* It represents a storage resource in the cluster (like a disk, network file system, etc.).

* PVs can be backed by: Google Persistent Disk (GCP), Amazon EBS (AWS), Azure Disk (Azure)

* A Persistent Volume Claim is a request for storage by a pod. It allows a pod to "claim" storage without knowing the underlying storage details.

* A PVC defines: Storage size, Access mode (e.g., ReadWriteOnce), Storage class (if applicable)

* Admin creates a Persistent Volume (PV). A pod requests storage using a Persistent Volume Claim (PVC).

* Kubernetes looks for a suitable PV that matches the request in the PVC. If a match is found, Kubernetes binds the PVC to the PV or else new PV will be created by vvthe cluster and PVC will bound to it. The pod mounts the PV and uses it for storage.


  
*********************************************************************** ConfigMaps ***************************************

* ConfigMaps are used to store and manage configuration data separately from application code. 

* This allows you to keep your application configuration flexible and environment-specific without modifying the containerized application itself.

* In configmaps we store insensitive information like Hostname, database name .

* K8s secret is used to store sensitive information like database-username and database-password.
  
  

************************************************************************ Volumes and Volume mount ********************************


* Volume requests a space in k8s pod to store configmaps, secrets, files, etc,.

* Volume mount is I have storage(Volume) I want to put it inside my container at this specific folder path.

* Ex:
 volumes:
  - name: my-volume
    configMap:
      name: my-config

 containers:
  - name: my-container
    image: my-app
    volumeMounts:
      - name: my-volume
        mountPath: /app/config




************************************************************************ Reverse proxy *****************************************

* A reverse proxy is a server that sits between clients (e.g., browsers, frontend apps) and backend servers (e.g., APIs, databases). It forwards client requests to the appropriate backend service and then returns the response to the client.

* Think of it as a middleman that helps secure, optimize, and route traffic between users and backend services.

* A client (browser, frontend app) sends a request (e.g., GET /api/all).

* The reverse proxy receives the request instead of the backend.

* It forwards the request to the appropriate backend server (e.g., http://student-app:8080/all).

* The backend processes the request and sends a response back to the proxy.

* The reverse proxy returns the response to the client.



************************************************************************** probes *******************************************************

* In Kubernetes, probes are used to monitor the health and readiness of containers in a Pod. They help ensure your application runs reliably by restarting unhealthy containers or preventing traffic from being sent to containers that aren't ready.

* k8s liveness probs let k8s know wheather our application is running in a container inside a pod is healthy or not .

* If our application is healthy , k8s will not involve in pod functioning , if our application is unhealthy k8s will marks it as unhealthy.

* If a pod fails health-checks countinously , then k8s will kill the container and restarts .

* In short k8s liveness pod will remove the unhealthy pods.

* kubelet will send http req every to seconds and expect a response.

* If the pod responds the http code between 200-399 then kubelt consider the container as healthy. if not it marks as unhealthy.

* We can mention the failure threshold , that many times the health check fails then kubelet will kill the container and restarts the pod.

* k8s Startup probe let k8s knows wheather our application in container inside pod is started or not .

* startup probe has higher priority over readiness and liveness probes . until the startup probe succeed the remaining probes will be in the disable state.

* startup probes are useful in the situation where our apps can take longer time to start or could fail while start .

* If the startup probe never succeed then the conatiner will be killed after 300 seconds and subject to the pod restart policy.

* A Kubernetes (k8s) readiness probe is a mechanism used by Kubernetes to determine whether a container is ready to accept traffic.

* If the readiness check fails, the container is temporarily removed from the Service's endpoints (i.e., it won’t receive any traffic).

* Unlike a liveness probe (which checks if the container is alive or should be restarted), a readiness probe only affects traffic routing.




****************************************************************** Limits and Requests ***********************************************

* Requests are what the container is guranteed to get .

* If a container requests a resources, k8s will only schedules that node which can give that resources.

* Limit make sures that container never goes above that value.

* The container is allowed to go upto that limit, then it is restricted.

* cpu and memory collectively called as resources or compute resources in k8s .

* 128 Mi (MebiBytes) is equal to 135 MB (megaBytes)

* 1000 m (milliCpus) is equal to 1 VCPU core

  
  

*********************************************************************  k8s commnads  ********************************************

* Kubectl get pods . To see the list of all pods.

* kubectl get pods -o wide . To see more info about pods.

* kubectl describe pod pod-name . To see the info of a particular pod.

* kubectl run nginx --image=nginx . To  create a pod .

* kubectl create deployment nginx --image=nginx . To create deployment in imperative way.

* kubectl delete pod pod-name . Deletes the pod .

* kubectl create -f replicaset.yml . To apply changes for existing rs.

* kubectl edit rs rs-name . To edit the configured rs .

* kubectl create deployment deployment-name --image=image-name --replicas=no-of-replicas . To create a deployment .

* Ex: kubectl create deployment webapp --image=nginx --replicas=3 .

* kubectl create -f deployment.yml . To create deployment.

* kubectl apply -f deployment.yml . To update the deployment.

* kubectl rollout status deployment/myapp-deployment. To check the status of deployment.

* kubectl rollout history deployment/myapp-deployment. To check the history of deployment 

* kubectl rollout undo deployment/myapp-deployment. To rollback the deployment.

* kubectl set image deploy deployment-name container-name=new-image-name . To change the image in deployment.

* kubect describe deployment deployment-name . To check the info about deployment.

* for pods ready state 2/2 means container running in pod to the total container is pod.s user in MySQL DB

* Connect to MySQL Database using kubectl -> kubectl run -it --rm --image=mysql:8.0 --restart=Never mysql-client -- mysql -h <Kubernetes-ClusterIP-Service> -u <USER_NAME> -p<PASSWORD>

* Replace ClusterIP Service, Username and Password -> kubectl run -it --rm --image=mysql:8.0 --restart=Never mysql-client -- mysql -h mysql -u root -pdbpassword11

* kubectl port-forward svc/student-app 8080:8080 -> It will forward the request to localhost

* 
 


********************************************************* Auto Scaling *****************************************************

* Thera are 2 types of autoscaling 1.Vertical AutoScaling , 2.Horizontal AutoScaling

* In vertical autoscaling we will increase the memory size and cpu size. But this is not used in many scenearois.

* In Horizontal pod autoscaling(HPA) we will add the extra vms or servers , we can say we add replicas .



************************************************* Yaml files ***********************************************

* Api versions for different kinds

      Kind          ApiVersiom
      
      pods              v1
      services          v1
      Deployment       apps/v1
      Replicasets      apps/v1
      ingress          networking.k8s.io/v1
      
      
* To link a deployment and a service  copy the selector, spec-> template-> metadata-> labels-> app part of deployment into selectors part of a service.

* In docker we use networks to communicate with in containers. In k8s we services to communicate with pods.





********************************************************** Deployment stratagies *****************************************

* Rolling Update (Default Strategy): The default deployment strategy, where Kubernetes gradually replaces old Pods with new ones without downtime.

* Recreate :  With this strategy, Kubernetes deletes all the old Pods before creating new ones.

* Blue/Green Deployment: This is not a native strategy in Kubernetes, but it can be implemented using separate Deployments and Services. The idea is to deploy the new version of an application alongside the old one, then switch traffic from the old version (blue) to the new version (green).

* Canary Deployment: A Canary deployment gradually rolls out the new version of the application to a small subset of users or Pods before rolling it out to the rest.

* Shadow Deployment: A shadow deployment sends real production traffic to the new version of the application but does not affect the actual user-facing application. This allows testing in production-like conditions without impacting users.





********************************************************* TroubleShooting k8s ***********************************

* ImagePullBackOff :
  
  -> The main reason for that error is Invalid image name or non-existing image and permision denied.

  -> From private registry like acr,ecr and gitlab registry we cannot pull the image without authentication for that we need to add imagepullpolicy in deployment.yml file .

  -> The first error for ImagePullBackOff is ErrImagePull because initially, Kubernetes tries to pull the image at specific time intervals. During this time, it shows the ErrImagePull error. Once the time exceeds and the retries fail, it changes to ImagePullBackOff.

* CrashLoopBackOff : 

  -> When you see "CrashLoopBackOff," it means that kubelet is trying to run the container, but it keeps failing and crashing. After crashing, Kubernetes tries to restart the container automatically, but if the container keeps failing repeatedly, you end up in a loop of crashes and restarts, thus the term "CrashLoopBackOff."

  -> There are many reasons for crashLoopBackOff :

  -> Misconfigurations can encompass a wide range of issues, from incorrect environment variables to improper setup of service ports or volumes.

  -> Liveness probes in Kubernetes are used to check the health of a container. If a liveness probe is incorrectly configured, it might falsely report that the container is unhealthy, causing Kubernetes to kill and restart the container repeatedly. 

  -> If the memory limits set for a container are too low, the application might exceed this limit, especially under load, leading to the container being killed by Kubernetes. 

  -> Containers might be configured to start with specific command-line arguments. If these arguments are wrong or lead to the application exiting (for example, passing an invalid option to a command), the container will exit immediately. 

  -> Bugs in the application code, such as unhandled exceptions or segmentation faults, can cause the application to crash. For instance, if the application tries to access a null pointer or fails to catch and handle an exception correctly, it might terminate unexpectedly.

* Resource sharing is also a common issues in k8s like suppose there are 5 namespaces and total memory is 100gb , one of the namespace is using 40gb and only 60 gb is left for remaining namespaces.

* This will happen because of memeory leak from specific pod .

* To avoid the these memeory issues use resource quota, it will allocate specific memory to each namespace and beyond that memory namespace unable to use that .

* And also add resource limits to pods.

* Container exceeds CPU limit	              Running (throttled)
* Container exceeds memory limit	      OOMKilled → CrashLoopBackOff
* Node under memory/disk/inode pressure	      Evicted

* Exec into a Specific Container

-> kubectl exec -it <pod-name> -c <container-name> -- /bin/sh

* If the cpu limit exceeds then it will not show the pod is crashed , it will slow down the application , like giving late response.

* Look for pods in CrashLoopBackOff, OOMKilled, or Evicted status — CPU throttling won’t crash the pod but may affect performance.

* To check the cpu usage of the specific pod -> kubectl top pod <pod-name> -n <namespace>






********************************************************* Trafic routes in k8s ********************************

* web -> ingress -> services -> pod (Based on labels) 

* 
 
********************************************************* ports *********************************************

* containerPort : The port exposed by the container (inside the Pod)
* port : The port on the Service object
* targetPort : The port the service routes to (inside the pod)
* nodePort : Exposes service on a static port on each node

* Ex: apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
    - port: 80          # Port on the Service
      targetPort: 8080  # Port on the Pod (containerPort)
      nodePort: 30080   # Port on the Node (optional)

* container port in deployment should match with Traget port in service.

-> Here's what this means:

Container	8080	Where the app runs inside the pod
Service	80	Exposes the app to other K8s services
NodePort	30080	Exposes to external traffic (cluster node)





************************************************ Helm *****************************************************
     
* Helm is a package manager for Kubernetes. It simplifies application deployment by allowing you to define, install, and upgrade Kubernetes applications using Helm charts (pre-configured application resources). It reduces YAML repetition and makes deployments reusable and manageable.

* Chart.yaml: Metadata about the chart (name, version, description)

* values.yaml: Default configuration values

* templates/: Contains Kubernetes resource templates (Deployment, Service, etc.)

* helm install: Deploys a new release

* helm upgrade: Applies changes to an existing release     
      
* Get revision list with: helm history <release-name>

* helm rollback <release-name> <revision-number>

* Use helmify to convert the kubernetes manifest into helm charts

* 1. pwd should the deployment-service.yaml files

* 2. Run cat *.yaml | helmify helm-chart-folder-name

* 3. Check the files.

* Helm packages templates into charts, supports versioning, upgrades, rollbacks, and value overrides — making it better for managing complex apps.

* Helm create folder-name will create the values.yaml, chart.yaml, templates and charts

* mychart/
├── Chart.yaml            # Required: Metadata about the chart (name, version, etc.)
├── values.yaml           # Default configuration values for the chart
├── charts/               # (Optional) Subcharts directory
├── templates/            # Contains Kubernetes manifest templates
│   ├── deployment.yaml   # Template for Deployment
│   ├── service.yaml      # Template for Service
│   ├── ingress.yaml      # (Optional) Template for Ingress
│   ├── _helpers.tpl      # (Optional) Template helpers (reusable snippets)
│   └── NOTES.txt         # (Optional) Instructions printed after install
└── .helmignore  





***************************************** Imp points ***********************************************

* There are two main reasons to deploy the application using k8s instaed of docker 

-> Auto healing 

-> Auto scaling 

* kubectl get all will display all the resources running in k8s cluster.

* What happens if a Pod resource needs to grow beyond the assigned limits?

-> If a Pod’s memory consumption exceeds its assigned memory limit, Kubernetes immediately kills the container with an out of memory (OOM) error. The container restarts if a restart policy is defined.
Unlike memory, if a Pod exceeds its assigned CPU limit, it is not killed. Instead, Kubernetes throttles CPU usage, causing the application to slow down.

* Init Containers are special containers that run before the main application containers in a pod start.

-> Example: If a db is running as a pod , in the main application we will add the db as init container to tell the db containers as started or not.

-> They are designed to perform initialization tasks, such as setting up configuration files, initializing databases, or any other operation necessary for the proper functioning of the application.

-> Init Containers run sequentially and complete their tasks before the primary containers start.

-> If an Init Container fails to execute successfully, the entire pod initialization fails, and the pod restarts until the Init Containers complete successfully.

* Sidecar Containers are additional containers that run alongside the main application container within the same pod.

* They extend or enhance the functionality of the primary container by providing supporting processes, utilities, or services.

* Taints and Tolerations in Kubernetes are mechanisms that control which pods can run on which nodes.

* Taint on Node = “I don’t want guests unless they have a pass.”

* Toleration on Pod = “I have the pass to enter.”

* Taints → set restrictions on nodes.

* Tolerations → allow pods to bypass those restrictions.





********************************************************************* Questions *********************************************


* We have kubectl why we use helm to deploy 

* how to debug a pod in pending state

* Difference between a pod and a container

* Explain the steps in dockerfile 

* Explain the cicd pipeline for the project 

* 


***************************************************************** Blue-Green Deployment implementation **************************************

1. Create Two Deployments

Blue Deployment → Current version (v1).

Green Deployment → New version (v2) you want to release.

Example:

# blue-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-blue
  labels:
    app: myapp
    version: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: blue
  template:
    metadata:
      labels:
        app: myapp
        version: blue
    spec:
      containers:
      - name: myapp
        image: myapp:v1
        ports:
        - containerPort: 80


# green-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-green
  labels:
    app: myapp
    version: green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: green
  template:
    metadata:
      labels:
        app: myapp
        version: green
    spec:
      containers:
      - name: myapp
        image: myapp:v2
        ports:
        - containerPort: 80

2. Expose Using a Service

Create a Service that points to only one version at a time.

# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
    version: blue   # 🔹 Initially routing to Blue
  ports:
    - port: 80
      targetPort: 80
  type: LoadBalancer

3. Deploy Blue First
kubectl apply -f blue-deployment.yaml
kubectl apply -f service.yaml


At this point, the app is live with version v1.

4. Deploy Green (New Version)
kubectl apply -f green-deployment.yaml


Now, both blue and green versions are running, but traffic is still going to blue.

5. Test Green Deployment Internally

You can test the green pods without exposing them to users:

kubectl get pods -l app=myapp,version=green
kubectl port-forward <green-pod-name> 8080:80


Then visit:

http://localhost:8080

6. Switch Traffic to Green

Update the Service selector from blue to green:

kubectl patch service myapp-service -p '{"spec":{"selector":{"app":"myapp","version":"green"}}}'

or 

Go to the service.yaml file and change the version to green and  then use kubectl apply -f service.yaml

7. Clean Up Old Version

Once verified, you can scale down or delete the blue deployment:

kubectl delete deployment myapp-blue

🔹 Benefits

Zero downtime switch.

Easy rollback → just switch service selector back to blue.










